{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c30000bc",
   "metadata": {},
   "source": [
    "# Rubik's Cube 2x2 Training Analysis\n",
    "\n",
    "This notebook analyzes the HRM model training for 2x2 Rubik's cube solving.\n",
    "\n",
    "## Current Status\n",
    "\n",
    "### Two Approaches\n",
    "1. **Policy Network** (dev branch): Predicts full solution sequence\n",
    "2. **Heuristic Network** (heuristic branch): Learns distance-to-solved for A* search\n",
    "\n",
    "### Key Findings\n",
    "- Small fully-connected network (4 layers, 512 nodes) with **one-hot encoding** works much better\n",
    "- Can train on just **1000 states** and get useful heuristic (2500 nodes avg during search)\n",
    "- HRM (recurrent) might be overkill for heuristic (single value prediction)\n",
    "- **Overfitting issue**: Good on train, poor on test ‚Üí dataset bias suspected\n",
    "\n",
    "### Dataset Bias Problem\n",
    "States closer to solved are **over-represented** because dataset is generated from solution sequences:\n",
    "- Each scramble creates a solution sequence\n",
    "- States early in solution (closer to solved) appear more often\n",
    "- Model learns to predict low distances too frequently\n",
    "\n",
    "## Goals\n",
    "1. Analyze dataset distribution\n",
    "2. Test different input encodings (state string vs one-hot)\n",
    "3. Compare HRM vs simple MLP performance\n",
    "4. Fix overfitting and improve generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dcfeb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import sys\n",
    "\n",
    "# Add project root\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "import py222\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd097b9c",
   "metadata": {},
   "source": [
    "## 1. Load and Analyze Heuristic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37aba7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Heuristic dataset not found. Run: python dataset/build_2x2_heuristic.py\n"
     ]
    }
   ],
   "source": [
    "def load_dataset_split(data_dir: Path, split: str):\n",
    "    \"\"\"Load a dataset split (train/test/val)\"\"\"\n",
    "    split_dir = data_dir / split\n",
    "    if not split_dir.exists():\n",
    "        raise FileNotFoundError(f\"Split {split} not found in {data_dir}\")\n",
    "\n",
    "    # Load metadata\n",
    "    with open(split_dir / \"dataset.json\", \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    # Load arrays\n",
    "    inputs = np.load(split_dir / \"all__inputs.npy\")\n",
    "    labels = np.load(split_dir / \"all__labels.npy\")\n",
    "    puzzle_indices = np.load(split_dir / \"all__puzzle_indices.npy\")\n",
    "\n",
    "    return {\n",
    "        'metadata': metadata,\n",
    "        'inputs': inputs,\n",
    "        'labels': labels.squeeze(),  # Remove extra dimension\n",
    "        'puzzle_indices': puzzle_indices,\n",
    "    }\n",
    "\n",
    "# Load heuristic dataset\n",
    "data_dir = Path(\"data/cube-2-by-2-heuristic\")\n",
    "if data_dir.exists():\n",
    "    train_data = load_dataset_split(data_dir, \"train\")\n",
    "    test_data = load_dataset_split(data_dir, \"test\")\n",
    "    \n",
    "    print(f\"Train samples: {len(train_data['inputs'])}\")\n",
    "    print(f\"Test samples: {len(test_data['inputs'])}\")\n",
    "    print(f\"Input shape: {train_data['inputs'].shape}\")\n",
    "    print(f\"Label shape: {train_data['labels'].shape}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Heuristic dataset not found. Run: python dataset/build_2x2_heuristic.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7237280d",
   "metadata": {},
   "source": [
    "## 2. Analyze Distance Distribution (BIAS CHECK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b4b1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_dir.exists():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Train distribution\n",
    "    axes[0].hist(train_data['labels'], bins=range(int(train_data['labels'].max()) + 2), \n",
    "                 edgecolor='black', alpha=0.7)\n",
    "    axes[0].set_title('Train: Distance to Solved Distribution')\n",
    "    axes[0].set_xlabel('Distance (moves)')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Test distribution\n",
    "    axes[1].hist(test_data['labels'], bins=range(int(test_data['labels'].max()) + 2), \n",
    "                 edgecolor='black', alpha=0.7, color='orange')\n",
    "    axes[1].set_title('Test: Distance to Solved Distribution')\n",
    "    axes[1].set_xlabel('Distance (moves)')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"\\nüìä Distribution Statistics:\")\n",
    "    print(f\"Train - Mean: {train_data['labels'].mean():.2f}, Median: {np.median(train_data['labels']):.0f}, Std: {train_data['labels'].std():.2f}\")\n",
    "    print(f\"Test  - Mean: {test_data['labels'].mean():.2f}, Median: {np.median(test_data['labels']):.0f}, Std: {test_data['labels'].std():.2f}\")\n",
    "    \n",
    "    # Check for bias\n",
    "    print(\"\\n‚ö†Ô∏è BIAS CHECK:\")\n",
    "    train_low = (train_data['labels'] <= 3).sum() / len(train_data['labels']) * 100\n",
    "    test_low = (test_data['labels'] <= 3).sum() / len(test_data['labels']) * 100\n",
    "    print(f\"States within 3 moves of solved: Train={train_low:.1f}%, Test={test_low:.1f}%\")\n",
    "    \n",
    "    if train_low > 40:\n",
    "        print(\"üî¥ SEVERE BIAS: >40% of states are close to solved!\")\n",
    "    elif train_low > 25:\n",
    "        print(\"üü° MODERATE BIAS: Dataset skewed toward easier states\")\n",
    "    else:\n",
    "        print(\"üü¢ Distribution looks reasonable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff94df4",
   "metadata": {},
   "source": [
    "## 3. Compare Input Encodings\n",
    "\n",
    "Test which encoding works better:\n",
    "1. **State String**: Current approach (24 integers 0-5)\n",
    "2. **One-Hot**: Each position ‚Üí 6-dimensional vector (24 √ó 6 = 144 dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1534a888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_onehot(state, num_colors=6):\n",
    "    \"\"\"Convert state array to one-hot encoding\"\"\"\n",
    "    # state: (batch, 24) ‚Üí (batch, 24, 6) ‚Üí (batch, 144)\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    if isinstance(state, np.ndarray):\n",
    "        state = torch.from_numpy(state)\n",
    "    \n",
    "    onehot = F.one_hot(state.long(), num_classes=num_colors)\n",
    "    return onehot.reshape(onehot.shape[0], -1).float()\n",
    "\n",
    "# Example\n",
    "if data_dir.exists():\n",
    "    sample_states = train_data['inputs'][:5]\n",
    "    print(\"Original encoding (first sample):\")\n",
    "    print(sample_states[0])\n",
    "    print(f\"Shape: {sample_states.shape}\")\n",
    "    \n",
    "    onehot_sample = state_to_onehot(sample_states)\n",
    "    print(\"\\nOne-hot encoding (first sample):\")\n",
    "    print(onehot_sample[0].numpy())\n",
    "    print(f\"Shape: {onehot_sample.shape}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ One-hot encoding: 24 positions √ó 6 colors = 144 features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0edded7",
   "metadata": {},
   "source": [
    "## 4. Simple MLP Baseline (What Your Colleague Found Works Best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe7d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    \"\"\"Simple 4-layer MLP with one-hot encoding\n",
    "    \n",
    "    This is what your colleague found works well:\n",
    "    - 4 fully connected layers\n",
    "    - 512 hidden units\n",
    "    - One-hot input encoding\n",
    "    - Trains in ~5 min on CPU\n",
    "    - Achieves ~350 nodes average during search\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=144, hidden_dim=512, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, 24) state indices\n",
    "        # Convert to one-hot\n",
    "        x_onehot = F.one_hot(x.long(), num_classes=6).float()\n",
    "        x_onehot = x_onehot.reshape(x_onehot.shape[0], -1)  # (batch, 144)\n",
    "        \n",
    "        x = F.relu(self.fc1(x_onehot))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)  # Output: distance estimate\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "# Create model\n",
    "simple_model = SimpleMLP()\n",
    "print(simple_model)\n",
    "print(f\"\\nParameters: {sum(p.numel() for p in simple_model.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "if data_dir.exists():\n",
    "    sample_input = torch.from_numpy(train_data['inputs'][:4])\n",
    "    with torch.no_grad():\n",
    "        output = simple_model(sample_input)\n",
    "    print(f\"\\nTest output shape: {output.shape}\")\n",
    "    print(f\"Sample predictions: {output.numpy()}\")\n",
    "    print(f\"True labels: {train_data['labels'][:4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a1910c",
   "metadata": {},
   "source": [
    "## 5. Quick Training Test (CPU-friendly)\n",
    "\n",
    "Train a small model on CPU to verify everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4abab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_simple_model(model, train_data, test_data, epochs=10, lr=1e-3, batch_size=64, device='cpu'):\n",
    "    \"\"\"Quick training function for simple MLP\"\"\"\n",
    "    \n",
    "    # Prepare data\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.from_numpy(train_data['inputs']),\n",
    "        torch.from_numpy(train_data['labels'].astype(np.float32))\n",
    "    )\n",
    "    test_dataset = TensorDataset(\n",
    "        torch.from_numpy(test_data['inputs']),\n",
    "        torch.from_numpy(test_data['labels'].astype(np.float32))\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    history = {'train_loss': [], 'test_loss': [], 'test_mae': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        test_mae = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                test_loss += criterion(outputs, targets).item()\n",
    "                test_mae += torch.abs(outputs - targets).mean().item()\n",
    "        \n",
    "        test_loss /= len(test_loader)\n",
    "        test_mae /= len(test_loader)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['test_mae'].append(test_mae)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Test Loss={test_loss:.4f}, Test MAE={test_mae:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Train on small subset for quick test\n",
    "if data_dir.exists():\n",
    "    print(\"Training simple MLP on small subset (1000 samples)...\")\n",
    "    small_train = {\n",
    "        'inputs': train_data['inputs'][:1000],\n",
    "        'labels': train_data['labels'][:1000]\n",
    "    }\n",
    "    small_test = {\n",
    "        'inputs': test_data['inputs'][:200],\n",
    "        'labels': test_data['labels'][:200]\n",
    "    }\n",
    "    \n",
    "    model = SimpleMLP()\n",
    "    history = train_simple_model(model, small_train, small_test, epochs=5, lr=1e-3)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['test_loss'], label='Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title('Training Progress')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['test_mae'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title('Test MAE (Lower is Better)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Final Test MAE: {history['test_mae'][-1]:.4f} moves\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8503ec36",
   "metadata": {},
   "source": [
    "## 6. Next Steps\n",
    "\n",
    "### Immediate Actions\n",
    "1. **Fix dataset bias**: \n",
    "   - Generate uniformly random scrambles (not from solutions)\n",
    "   - Or weight samples inversely to distance\n",
    "   \n",
    "2. **Test simple MLP vs HRM**:\n",
    "   - Train both on same data\n",
    "   - Compare speed, accuracy, and search performance\n",
    "   \n",
    "3. **Input encoding experiments**:\n",
    "   - One-hot (current best)\n",
    "   - Learned embeddings\n",
    "   - Relative position encoding\n",
    "\n",
    "### For Presentation\n",
    "- Dataset distribution analysis (this notebook)\n",
    "- Model architecture comparison (HRM vs MLP)\n",
    "- Search performance metrics (nodes expanded, solution length)\n",
    "- Visualizations of cube solving\n",
    "- Comparison to baselines (BFS, DeepCubeA)\n",
    "- Extension to 3x3 (if time permits)\n",
    "\n",
    "### Code Cleanup\n",
    "- Refactor solver.py with Hydra CLI\n",
    "- Add more natural comments\n",
    "- Clean up AI-generated code sections"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
